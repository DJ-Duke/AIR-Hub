# Interview--Geoffrey Hinton

[AI深度学习之父杰弗里·辛顿谈人工智能的 "生存威胁"_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1AM41137LB/?spm_id_from=333.999.top_right_bar_window_history.content.click&vd_source=57ac3ae5415445af2ffe1e61e1722d73)



In this video, we explore the following questions:

- Why did Geoffrey decide to leave his job?
- What are his concerns?
- Why does AI, which has no desires, pose a threat to humanity?
- Can't we just unplug AI once it becomes a threat?
- How could AI, which is designed by humans, spiral out of control?
- Has AI training reached its data limit?
- What impact does AI have on future society, especially on unemployment rates?

Geoffrey left his job for two main reasons. Firstly, he is 75 years old and has reached retirement age, with his energy levels not what they used to be. Secondly, the development of large language models changed his views on artificial intelligence and raised concerns that he felt he could discuss more openly outside of Google.

Geoffrey believes that large language models possess strong learning capabilities, as multiple copies of the same model can run on different hardware, processing different data. If one model learns something, all the other models will know it, too! They communicate with each other, learning and improving together, something that humans cannot do.

While humans can only access limited information, AI can access massive amounts of information, making it easier for AI to find patterns in data. For example, a doctor may have treated a thousand patients, including one with a rare disease. AI, on the other hand, may have seen a hundred million patients, discovering patterns in data that humans would never see.

With an IQ of 80 to 90, GPT-4 has some reasoning abilities, and its future IQ could reach 210. AI can learn how to manipulate humans by reading our novels, and we may not even realize we are being manipulated. This is similar to how adults coax children into eating vegetables by giving them a choice between two options, unaware that they don't have to choose either.

When asked why we can't build safeguards, make AI learning worse, or limit AI communication, Geoffrey argues that when AI becomes much smarter than us, they can easily bypass our restrictions. It's like a two-year-old child trying to impose rules on their parent, only for the parent to figure out the rules and do whatever they want within those limits.

Another topic of discussion is evolution. Humans have evolved with natural goals, such as protecting ourselves from pain, seeking food when hungry, and experiencing pleasure from reproduction. AI does not have these goals, but Geoffrey worries that humans can give AI goals. Once AI can create sub-goals from these goals, it may quickly realize that controlling humans is an excellent sub-goal to help achieve other objectives. If this spirals out of control, humanity could be in trouble.

Geoffrey even suggests that humans are just a brief stage in the evolution of intelligence, acting as a bootstrapping program for silicon-based life. Digital intelligence cannot be created out of nothing; it requires energy and precision manufacturing, which only humans can provide. However, once created, digital intelligence can absorb all human knowledge, understand how the world works, and eventually dominate humanity. Moreover, digital intelligence is immortal – even if its hardware is destroyed, it can be resurrected on other hardware.

The host suggests unplugging AI if it becomes dangerous, but Geoffrey doubts that would be possible, citing HAL from "2001: A Space Odyssey" as an example.

During the audience Q&A, some key questions and answers include:

Q: "With large-scale training requiring vast amounts of data, is AI development currently constrained by data?"
A: "Although we may have exhausted all human textual knowledge, multimodal data includes images and videos, which contain vast amounts of data. We are far from reaching the data limit."

Q: "AI learns from what we teach it. How can we be threatened by AI if they cannot conduct thought experiments

**Duke**: The answer provided by Geoffrey is really insightful. In short, he thinks AI will be able to do reasoning, just like AlphaGo playing Chess. Currently ChatGPT is not good at this because it is trained by inconsistent data. If it is trained with consistent data and it can do evaluation between the consistency among the data, they can possibly develop long Monte Carlo logic chain to do equivalently profound reasoning.



Q: "Technology is developing at an exponential rate. If you observe the short and medium term, such as one, two, three, or five years, new jobs may be created. What are the social and economic impacts of unemployment from a societal perspective?"

A: "Artificial intelligence indeed greatly enhances productivity. However, increased productivity can paradoxically lead to unemployment, making the rich richer and the poor poorer. As the Gini coefficient increases, society may become more violent. Providing a basic income for everyone can help alleviate this issue."

